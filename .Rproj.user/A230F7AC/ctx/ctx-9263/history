printTab[, -c(1:3)] <- round(printTab[, -c(1:3)] * 100)
kbl(rbind(printTab[which(printTab$pow.i == 5/4), -3],
c("Wins", "", colSums(rankings[which(rankings$pow.i == 5/4), 4:ncol(rankings)] > cutoff)))
, format = "latex", booktabs = T, align = "c",
linesep = c(rep("", 2), "\\addlinespace"), digits = 0, escape = F,
col.names = c("Dist", "p", rep(c("$T_1$", "$T_2$","$T_\\infty$"), times = 4)), row.names = F) %>%
add_header_above(c(" " = 2, "Clipped" = 3, "Moment" = 3,
"Tanh" = 3, "Trig" = 3)) %>%
collapse_rows(columns = c(1), valign = "middle")
gFunc <- function(x, type){
if(type == 1){
return(scale(x^2))
} else if (type ==2 ){
return(scale(x^3))
} else if (type ==3 ){
return(scale(abs(x)^(2.5) * sign(x)))
} else if (type == 4){
return(scale(tanh(x)))
} else if (type == 5){
return(scale(tanh(x) * x))
} else if(type == 6){
return(scale(tanh(x) * abs(x)^(1.5)))
} else if (type == 7){
return(scale(x^2 / (1 + x^2)))
} else if (type == 8){
return(scale(tanh(x)))
} else if (type == 9){
return(scale(sin(x * rnorm(1))))
} else if (type == 10){
return(scale(cos(x * rnorm(1))))
}
}
library("tidyverse")
library("kableExtra")
kbl(rbind(printTab[which(printTab$pow.i == 5/4), -3],
c("Wins", "", colSums(rankings[which(rankings$pow.i == 5/4), 4:ncol(rankings)] > cutoff)))
, format = "latex", booktabs = T, align = "c",
linesep = c(rep("", 2), "\\addlinespace"), digits = 0, escape = F,
col.names = c("Dist", "p", rep(c("$T_1$", "$T_2$","$T_\\infty$"), times = 4)), row.names = F) %>%
add_header_above(c(" " = 2, "Clipped" = 3, "Moment" = 3,
"Tanh" = 3, "Trig" = 3)) %>%
collapse_rows(columns = c(1), valign = "middle")
kbl(rbind(printTab[which(printTab$pow.i == 2), -3],
c("Wins", "", colSums(rankings[which(rankings$pow.i == 2), 4:ncol(rankings)] > cutoff)))
, format = "latex", booktabs = T, align = "c",
linesep = c(rep("", 2), "\\addlinespace"), digits = 0, escape = F,
col.names = c("Dist", "p", rep(c("$T_1$", "$T_2$","$T_\\infty$"), times = 4)), row.names = F) %>%
add_header_above(c(" " = 2, "Clipped" = 3, "Moment" = 3,
"Tanh" = 3, "Trig" = 3)) %>%
collapse_rows(columns = c(1), valign = "middle")
## Notes:
# Main analysis done on cluster using fama_cluster.R
# This is for post-processing the results of that analysis
# which is recorded in the fama_res2019.csv file
## Distance for comparing two orderings
# used for computing frechet mean
orderingComp <- function(o1, o2){
p <- length(o1)
temp <- 0
for(i in 1:(p-1)){
d1 <- o1[(i+1):p]
d2 <- o2[which(o2 == o1[i]):p]
temp <- temp + sum(!(d1 %in% d2))
}
return(temp)
}
# fama2019 <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/results/fama_res2019.csv")
fama2019 <- read.csv("~/confSetsNew/results/data_analysis/fama_res2018.csv")
fama2019 <- read.csv("~/confSetsNew/results/data_analysis/fama_res2018.csv")
fama2019 <- read.csv("~/confSetsNew/results/data_analysis/fama_max_res2018.csv")
fama2019 <- read.csv("~/confSetsNew/results/data_analysis/fama_max_res2018.csv")
fama2019 <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/results/fama_max_res2019.csv")
fama2019 <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/results/fama_max_res2018.csv")
# fama2019 <- read.csv("~/confSetsNew/results/data_analysis/fama_max_res2018.csv")
fama2019 <- as.matrix(data.frame(fama2019[, -1]))
A <- cdcs::getAncest(fama2019)
colnames(A) <- rownames(A) <- colnames(Y)
famaData <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/data/fama12.csv")
Y <- famaData[which(substr(famaData[,1], 1, 4) >= 2019), -1]
names.Y <- names(Y)
Y <- scale(Y)
A <- cdcs::getAncest(fama2019)
colnames(A) <- rownames(A) <- colnames(Y)
A
# Analyzing 2019-2022
fama2019 <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/results/fama_max_res2018.csv")
Y <- famaData[which(substr(famaData[,1], 1, 4) >= 2019), -1]
nrow(Y)
Y.centered <- scale(Y, scale = F)
ordering <- causalXtreme::direct_lingam_search(scale(Y))
names(Y)[ordering]
factorial(12) / nrow(fama2019)
max(fama2019[,1])
ancestMat <- cdcs::getAncest(fama2019)
ancestMat
famaData <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/data/fama12.csv")
# Analyzing 2019-2022
fama2019 <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/results/fama_max_res2019.csv")
Y <- famaData[which(substr(famaData[,1], 1, 4) >= 2019), -1]
nrow(Y)
Y.centered <- scale(Y, scale = F)
ordering <- causalXtreme::direct_lingam_search(scale(Y))
names(Y)[ordering]
factorial(12) / nrow(fama2019)
max(fama2019[,1])
ancestMat <- cdcs::getAncest(fama2019)
ancestMat <- round(ancestMat, 2)
ancestMat[upper.tri(ancestMat, diag = T)] <- ""
ancestMat
ancestMat <- cdcs::getAncest(fama2019)
ancestMat
factorial(12) / nrow(fama2019[which(fama2019[,1] > .05), ])
# Analyzing 2019-2022
fama2019 <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/results/fama_max_res2018.csv")
Y <- famaData[which(substr(famaData[,1], 1, 4) >= 2019), -1]
nrow(Y)
Y.centered <- scale(Y, scale = F)
ordering <- causalXtreme::direct_lingam_search(scale(Y))
names(Y)[ordering]
factorial(12) / nrow(fama2019[which(fama2019[,1] > .05), ])
max(fama2019[,1])
ancestMat <- cdcs::getAncest(fama2019[which(fama2019[,1] > .05), ])
ancestMat
nrow(fama2019[which(fama2019[,1] > .05), ])
# Analyzing 2019-2022
fama2019 <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/results/fama_max_res2019.csv")
Y <- famaData[which(substr(famaData[,1], 1, 4) >= 2019), -1]
nrow(Y)
Y.centered <- scale(Y, scale = F)
ordering <- causalXtreme::direct_lingam_search(scale(Y))
names(Y)[ordering]
factorial(12) / nrow(fama2019[which(fama2019[,1] > .05), ])
max(fama2019[,1])
ancestMat <- cdcs::getAncest(fama2019[which(fama2019[,1] > .05), ])
library(rdrobust)
n<-150
set.seed(100+732)
z<-2*rbeta(n,2,4)-1
y<- -1 + 0.581*z + 0.0060*z*1 - 0.058*z^2 + 0.01074+rnorm(n,0,0.1)
plot(z,y)
rd1<-rdrobust(y,z)
rep(1/n, n)
n
sum(rep(1/n, n)^2)
1/n
parallel::parLapply
## Distance for comparing two orderings
# used for computing frechet mean
orderingComp <- function(o1, o2){
p <- length(o1)
temp <- 0
for(i in 1:(p-1)){
d1 <- o1[(i+1):p]
d2 <- o2[which(o2 == o1[i]):p]
temp <- temp + sum(!(d1 %in% d2))
}
return(temp)
}
fama2019 <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/results/fama_max_res2019.csv")
fama2019 <- as.matrix(data.frame(fama2019[, -1]))
# Calculate Squared distance for all orderings
sumSq <- rep(0, nrow(fama2019))
p <- ncol(fama2019)
for(i in 1:(nrow(fama2019))){
temp <- 0
for(j in setdiff(1:nrow(fama2019), i)){
temp <- temp + orderingComp(fama2019[i,], fama2019[j, ])^2
}
sumSq[i] <- temp
if(i %% 100 == 0){
cat(i)
cat("/")
cat(nrow(fama2019))
cat(" ")
cat(i / nrow(fama2019))
cat("\n")
}
}
fama2019 <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/results/fama_max_res2018.csv")
fama2019 <- as.matrix(data.frame(fama2019[, -1]))
dim(fama2019)
fama2019 <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/results/fama_max_res2019.csv")
fama2019 <- as.matrix(data.frame(fama2019[, -1]))
dim(fama2019)
factorial(12)
fama2019 <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/results/fama_max_res2019.csv")
fama2019 <- as.matrix(data.frame(fama2019[, -1]))
# Calculate Squared distance for all orderings
sumSq <- rep(0, nrow(fama2019))
p <- ncol(fama2019)
for(i in 1:(nrow(fama2019))){
temp <- 0
for(j in setdiff(1:nrow(fama2019), i)){
temp <- temp + orderingComp(fama2019[i,], fama2019[j, ])^2
}
sumSq[i] <- temp
if(i %% 100 == 0){
cat(i)
cat("/")
cat(nrow(fama2019))
cat(" ")
cat(i / nrow(fama2019))
cat("\n")
}
}
j
temp
rec1
##################
# Supplement D
#
# Comparison of bootstrap procedure to the limiting Gaussian distribution
#
set.seed(1111)
p.list <- c(5, 10, 15, 20, 25)
sim.size <- 2000
rec1 <- rec2 <- rec3 <- rec4 <- matrix(0, sim.size, length(p.list))
rec2 <- matrix(0, sim.size, length(p.list))
rho <- .2
dist <- "ln"
bs <- 500
z <- log(1 + sqrt(5)) - log(2)
# par(mfrow = c(5,4))
for(z in 1:length(p.list)){
p <- p.list[z]
n <- 500
M <- diag(p) + matrix(rho, p, p) - diag(rep(rho, p))
beta <- rep(1, p)
cat(p)
cat(": ")
for(i in 1:sim.size){
if(i %% 100 == 0){
cat(i)
cat(" ")
}
X <- scale(exp(mvtnorm::rmvnorm(n, sigma = M)))
Y <- X %*%  beta + exp(rnorm(n, sd = sqrt(z))) - exp(z/2)
scale.X2 <- scale(X^2)
# X <- lcmix::rmvgamma(n, corr = M)
# Y <- X %*%  beta + rgamma(n, 2, 1) - 2
# scale.X2 <- scale(X^2)
# X <- scale(LaplacesDemon::rmvl(n, mu = rep(0, p), Sigma = M))
# Y <- X %*%  beta + rlaplace(n, 0, 1)
# scale.X2 <- scale(X^3)
X.int <- cbind(rep(1, n), X)
H.mat <- t(scale.X2) %*% (diag(n) - X.int %*% solve(t(X.int) %*% X.int, t(X.int)))
res <- RcppArmadillo::fastLm(X = X.int, y = Y)$res
sd.est <- sum(res^2/(n-p))
stat <- sum((t(scale.X2) %*% res / sqrt(n))^2)
# true.sd <- sqrt((exp(1) - 1) * exp(1))
nullDist1 <- replicate(bs, sum((H.mat %*% rnorm(n, 0, sd = sqrt(sd.est)) / sqrt(n))^2))
# nullDist2 <- replicate(500, sum((H.mat %*% rnorm(n, 0, sd = sqrt(sd.est)) / sqrt(n-p))^2))
nullDist3 <- replicate(500, sum((H.mat %*% (exp(rnorm(n, sd = sqrt(z))) - exp(z/2)) / sqrt(n))^2))
# nullDist3 <- replicate(500, sum((H.mat %*% rlaplace(n, 0, 1) / sqrt(n))^2))
### Null Hyp
# ind <- p
child <- Y
parents <- X
G <- array(0, dim = c(n , 1, ncol(parents)))
k <- 1
for(j in 1:ncol(parents)){
G[ , k, j] <- scale.X2[, j]
}
rec1[i, z] <- (sum(stat < nullDist1) + 1) / (bs + 1)
rec2[i, z] <- (sum(stat < (nullDist1 * n /(n-p))) +1) / (bs + 1)
rec3[i, z] <- (sum(stat < nullDist3) +1) / (bs + 1)
rec4[i, z] <- cdcs::bnbHelperanm(parents, child, G = G, withinAgg = 3, aggType = 3, bs = bs, intercept = 1)$pVals[1]
}
# cat("\n")
# hist(rec1[, z], freq = F, main = p, breaks = seq(0, 1, by = .05))
# mtext(round(mean(rec1[, z] < .05),3))
# hist(rec2[, z], freq = F, main = p, breaks = seq(0, 1, by = .05))
# mtext(round(mean(rec2[, z] < .05),3))
# hist(rec3[, z], freq = F, main = p, breaks = seq(0, 1, by = .05))
# mtext(round(mean(rec3[, z] < .05),3))
# hist(rec4[, z], freq = F, main = p, breaks = seq(0, 1, by = .05))
# mtext(round(mean(rec4[, z] < .05),3))
}
# colMeans(rec1 < .1)
# colMeans(rec2 < .1)
# colMeans(rec3 < .1)
# colMeans(rec4 < .1)
getColumnPrint <- function(x, alpha, mult = 1){
meanx <- colMeans(x <= alpha)
sdx <- sqrt(meanx * (1 - meanx) / nrow(x))
paste(round(meanx * mult), rep(" (", ncol(x)),
round((meanx - 2 * sdx) * mult), rep(", ", ncol(x)),
round((meanx + 2 * sdx) * mult), rep(")", ncol(x)), sep = "")
}
setEPS()
postscript("~/Dropbox/Apps/Overleaf/Confidence Sets for Causal Discovery/figures/whyAsympBad_max.eps", width = 6, height = 6)
par(mfrow = c(5,3), mar = c(1, 4, 4, 0))
for(z in 1:5){
# hist(rec1[, z], freq = F, main = p, breaks = seq(0, 1, by = .05), )
# mtext(round(mean(rec1[, z] < .05),3))
hist(rec2[, z], freq = F, main = paste("Asymp: p=", p.list[z], sep = ""), breaks = seq(0, 1, by = .05), xlab = "")
mtext(round(mean(rec2[, z] < .05),3))
hist(rec3[, z], freq = F, main = paste("Oracle: p= ", p.list[z], sep = ""), breaks = seq(0, 1, by = .05), xlab = "")
mtext(round(mean(rec3[, z] < .05),3))
hist(rec4[, z], freq = F, main = paste("Proposed: p= ", p.list[z], sep = ""), breaks = seq(0, 1, by = .05), xlab = "")
mtext(round(mean(rec4[, z] < .05),3))
}
dev.off()
tab <- rbind(getColumnPrint(rec2, .05, 1000),
getColumnPrint(rec3, .05, 1000),
getColumnPrint(rec4, .05, 1000))
rownames(tab) <- c("Asymp", "Oracle", "Proposed")
colnames(tab) <- p.list
xtable::xtable(tab)
knitr::opts_chunk$set(echo = TRUE)
# fit the lasso (alph = 1 indicates lasso) with a poisson family
lasso_mod <- glmnet::cv.glmnet(y = bike_data_train$bike_counts, x = as.matrix(bike_data_train[, -c(1,2)]),
alpha = 1, family = "poisson")
bike_data_train <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/bike_data_18.csv")
dim(bike_data_train)
names(bike_data_train)
# fit the lasso (alph = 1 indicates lasso) with a poisson family
lasso_mod <- glmnet::cv.glmnet(y = bike_data_train$bike_counts, x = as.matrix(bike_data_train[, -c(1,2)]),
alpha = 1, family = "poisson")
plot(lasso_mod)
# we can see the coefficients selected by the lambda value with the lowest CV error
coef(lasso_mod, s = lasso_mod$lambda.min)
# or we can see the coefficients selected by the largest lambda value with a CV error within 1 standard error of the lowest CV error
coef(lasso_mod, s = lasso_mod$lambda.1se)
# fit the lasso (alph = 1 indicates lasso) with a poisson family
lasso_mod <- glmnet::cv.glmnet(y = bike_data_train$bike_counts, x = as.matrix(bike_data_train[, -c(1,2)]),
alpha = 1, family = "poisson")
plot(lasso_mod)
# we can see the coefficients selected by the lambda value with the lowest CV error
coef(lasso_mod, s = lasso_mod$lambda.min)
# or we can see the coefficients selected by the largest lambda value with a CV error within 1 standard error of the lowest CV error
coef(lasso_mod, s = lasso_mod$lambda.1se)
# fit the ridge regression (alpha = 0 indicates ridge) with a poisson family
ridge_mod <- glmnet::cv.glmnet(y = bike_data_train$bike_counts, x = as.matrix(bike_data_train[, -c(1,2)]),
alpha = 0, family = "poisson")
plot(ridge_mod)
# we can see the coefficients selected by the lambda value with the lowest CV error
coef(ridge_mod, s = ridge_mod$lambda.min)
# or we can see the coefficients selected by the largest lambda value with a CV error within 1 standard error of the lowest CV error
coef(ridge_mod, s = ridge_mod$lambda.1se)
# fit the lasso (alph = 1 indicates lasso) with a poisson family
lasso_mod <- glmnet::cv.glmnet(y = bike_data_train$bike_counts, x = as.matrix(bike_data_train[, -c(1,2)]),
alpha = 1, family = "poisson")
plot(lasso_mod)
# we can see the coefficients selected by the lambda value with the lowest CV error
coef(lasso_mod, s = lasso_mod$lambda.min)
# fit the lasso (alph = 1 indicates lasso) with a poisson family
lasso_mod <- glmnet::cv.glmnet(y = bike_data_train$bike_counts, x = as.matrix(bike_data_train[, -c(1,2)]),
alpha = 1, family = "poisson")
plot(lasso_mod)
# or we can see the coefficients selected by the largest lambda value with a CV error within 1 standard error of the lowest CV error
coef(lasso_mod, s = lasso_mod$lambda.1se)
# fit the lasso (alph = 1 indicates lasso) with a poisson family
lasso_mod <- glmnet::cv.glmnet(y = bike_data_train$bike_counts, x = as.matrix(bike_data_train[, -c(1,2)]),
alpha = 1, family = "poisson")
plot(lasso_mod)
# or we can see the coefficients selected by the largest lambda value with a CV error within 1 standard error of the lowest CV error
coef(lasso_mod, s = lasso_mod$lambda.1se)
# fit the ridge regression (alpha = 0 indicates ridge) with a poisson family
ridge_mod <- glmnet::cv.glmnet(y = bike_data_train$bike_counts, x = as.matrix(bike_data_train[, -c(1,2)]),
alpha = 0, family = "poisson")
plot(ridge_mod)
# We can see the coefficients selected by the largest lambda value with a CV error within 1 standard error of the lowest CV error
coef(ridge_mod, s = ridge_mod$lambda.1se)
?plot..cv.glmnet
# fit the lasso (alph = 1 indicates lasso) with a poisson family
lasso_mod <- glmnet::cv.glmnet(y = bike_data_train$bike_counts, x = as.matrix(bike_data_train[, -c(1,2)]),
alpha = 1, family = "poisson")
plot(lasso_mod)
# or we can see the coefficients selected by the largest lambda value with a CV error within 1 standard error of the lowest CV error
coef(lasso_mod, s = lasso_mod$lambda.1se)
# fit the ridge regression (alpha = 0 indicates ridge) with a poisson family
ridge_mod <- glmnet::cv.glmnet(y = bike_data_train$bike_counts, x = as.matrix(bike_data_train[, -c(1,2)]),
alpha = 0, family = "poisson")
plot(ridge_mod)
# We can see the coefficients selected by the largest lambda value with a CV error within 1 standard error of the lowest CV error
coef(ridge_mod, s = ridge_mod$lambda.1se)
mean((bike_data_test$bike_counts - predict(ridge_mod, s=ridge_mod$lambda.min,  newx = as.matrix(bike_data_test[,-c(1,2)]), type = "response"))^2)
# Test data from 2019
bike_data_test <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/bike_data_19.csv")
# regular glm without any penalization
unpenalized_model <- glm(bike_counts ~ ., data = bike_data_train[, -1], family = "poisson")
# predictive accuracy for 2019 when using all covariates but no model seletion or penalization
## use type = "response" to get predictions in bikes, instead of log(bikes)
mean((bike_data_test$bike_counts - predict(unpenalized_model, newx = as.matrix(bike_data_test[,-c(1)]), type = "response"))^2)
## to get predictions for a penalized regression
## use type = "response" to get predictions in bikes, instead of log(bikes)
mean((bike_data_test$bike_counts - predict(ridge_mod, s=ridge_mod$lambda.min,  newx = as.matrix(bike_data_test[,-c(1,2)]), type = "response"))^2)
# Test data from 2019
bike_data_test <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/bike_data_19.csv")
# regular glm without any penalization
unpenalized_model <- glm(bike_counts ~ ., data = bike_data_train[, -1], family = "poisson")
# predictive accuracy for 2019 when using all covariates but no model seletion or penalization
## use type = "response" to get predictions in bikes, instead of log(bikes)
mean((bike_data_test$bike_counts - predict(unpenalized_model, newx = as.matrix(bike_data_test[,-c(1)]), type = "response"))^2)
## to get predictions for a penalized regression
## use type = "response" to get predictions in bikes, instead of log(bikes)
mean((bike_data_test$bike_counts - predict(lasso_mod, s=lasso_mod$lambda.1se,  newx = as.matrix(bike_data_test[,-c(1,2)]), type = "response"))^2)
mean((bike_data_test$bike_counts - predict(ridge_mod, s=ridge_mod$lambda.1se,  newx = as.matrix(bike_data_test[,-c(1,2)]), type = "response"))^2)
unpenalized_model <- glm(bike_counts ~ ., data = bike_data_train[, -1], family = "poisson")
unpenalized_model
unpenalized_model <- glm(bike_counts ~ ., data = bike_data_train[, -1], family = "poisson")
summary(unpenalized_model)
unpenalized_model <- glm(bike_counts ~ ., data = bike_data_train[, -1], family = "quasipoisson")
summary(unpenalized_model)
variance(bike_data_test$bike_counts)
var(bike_data_test$bike_counts)
mean((bike_data_test$bike_counts - predict(unpenalized_model, newx = as.matrix(bike_data_test[,-c(1)]), type = "response"))^2)
mean((bike_data_test$bike_counts - predict(unpenalized_model, newx = as.matrix(bike_data_test[,-c(1,2)]), type = "response"))^2)
bike_data_test
# Test data from 2019
bike_data_test <- read.csv("https://raw.githubusercontent.com/ysamwang/btry6020_sp22/main/lectureData/bike_data_19.csv")
# regular glm without any penalization
unpenalized_model <- glm(bike_counts ~ ., data = bike_data_train[, -1], family = "poisson")
# predictive accuracy for 2019 when using all covariates but no model seletion or penalization
## use type = "response" to get predictions in bikes, instead of log(bikes)
mean((bike_data_test$bike_counts - predict(unpenalized_model, newx = as.matrix(bike_data_test[,-c(1,2)]), type = "response"))^2)
## to get predictions for a penalized regression
## use type = "response" to get predictions in bikes, instead of log(bikes)
mean((bike_data_test$bike_counts - predict(lasso_mod, s=lasso_mod$lambda.1se,  newx = as.matrix(bike_data_test[,-c(1,2)]), type = "response"))^2)
mean((bike_data_test$bike_counts - predict(ridge_mod, s=ridge_mod$lambda.1se,  newx = as.matrix(bike_data_test[,-c(1,2)]), type = "response"))^2)
unpenalized_model
summary(unpenalized_model)
coef(lasso_mod)
# fit the lasso (alph = 1 indicates lasso) with a poisson family
lasso_mod <- glmnet::cv.glmnet(y = bike_data_train$bike_counts,
x = as.matrix(bike_data_train[, -c(1,2)]),
alpha = 1, family = "poisson")
plot(lasso_mod)
# We can see the coefficients selected by the largest lambda value with a CV error within 1 standard error of the lowest CV error
coef(lasso_mod, s = lasso_mod$lambda.1se)
fama2019 <- read.csv("~/confSets/results/data_analysis/fama_max_res2019_23.csv")
fama2019 <- read.csv("~/confSets/results/data_analysis/results/fama_max_res2019_23.csv")
fama2019 <- read.csv("~/confSets/results/data_analysis/results/fama_max_res2019_23.csv")
fama2019 <- read.csv("~/confSets/data_analysis/results/fama_max_res2019_23.csv")
fama2019 <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/results/fama_max_res2019_23.csv")
fama2019 <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/results/fama_max_res2019_23.csv")
fama2019
dim(~/Dropbox/confSetGraphs/code/rPkg/)
famaData <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/data/fama12.csv")
Y <- famaData[which(substr(famaData[,1], 1, 4) >= 2019), -1]
names.Y <- names(Y)
Y <- scale(Y)
sumSq <- as.matrix(read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/results/sumSq_2019_max_23.csv"))
hist(sumSq)
which.min(sumSq)
sumSq[which.min(sumSq)]
close <- which(sumSq <= sumSq[which.min(sumSq)] * 1.05)
rec <- rec1 <- rep(0, length(close))
for(i in 1:length(close)){
rec[i] <- orderingComp(fama2019[which.min(sumSq), -1], fama2019[close[i], -1])
rec1[i] <- orderingComp(fama2019[close[i], -1], fama2019[which.min(sumSq), -1])
}
A <- cdcs::getAncest(fama2019)
A <- cdcs::getAncest(fama2019)
install.packages("pcalg")
BiocManager::install("graph")
BiocManager::install("RBGL")
install.packages("pcalg")
library(cdcs)
A <- cdcs::getAncest(fama2019)
A
nrow(fama2019)
factorial(12)/nrow(fama2019)
orderingComp <- function(o1, o2){
p <- length(o1)
temp <- 0
for(i in 1:(p-1)){
d1 <- o1[(i+1):p]
d2 <- o2[which(o2 == o1[i]):p]
temp <- temp + sum(!(d1 %in% d2))
}
return(temp)
}
# fama2019 <- read.csv("~/confSets/results/data_analysis/fama_max_res2019_23.csv")
fama2019 <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/results/fama_max_res2019_23.csv")
fama2019 <- as.matrix(data.frame(fama2019[, -1]))
famaData <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/data/fama12_23.csv")
Y <- famaData[which(substr(famaData[,1], 1, 4) >= 2019), -1]
names.Y <- names(Y)
Y <- scale(Y)
sumSq <- as.matrix(read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/results/sumSq_2019_max_23.csv"))
hist(sumSq)
which.min(sumSq)
sumSq[which.min(sumSq)]
close <- which(sumSq <= sumSq[which.min(sumSq)] * 1.05)
rec <- rec1 <- rep(0, length(close))
for(i in 1:length(close)){
rec[i] <- orderingComp(fama2019[which.min(sumSq), -1], fama2019[close[i], -1])
rec1[i] <- orderingComp(fama2019[close[i], -1], fama2019[which.min(sumSq), -1])
}
A <- cdcs::getAncest(fama2019)
colnames(A) <- rownames(A) <- colnames(Y)
ZZ <- matrix(0, 12, 12)
ZZ[lower.tri(ZZ, diag = F)] <- 1
symA <- A + t(ZZ - A)
mat <- data.frame(rep(colnames(Y), times = 12), rep(colnames(Y), each = 12), c(symA))
names(mat) <- c("Descendant", "Ancestor", "Proportion")
symA.rearrange <- symA[colnames(Y)[fama2019[which.min(sumSq), ]], colnames(Y)[fama2019[which.min(sumSq), ]]]
rownames(symA.rearrange) <- colnames(symA.rearrange) <- c("Utl", "Whl", "NoDr", "Hlth", "Chem",
"Fin","Enrg", "BsEq", "Tel","Durb", "Mfg","Oth")
library(plot.matrix)
par(oma = c(0, 0, 0, 2), mar = c(.5, 4.5, 4, 2))
plot(symA.rearrange, col = c("white",RColorBrewer::brewer.pal(n = 9, "Blues")),
cex.axis = .8, las = 1, xlab = "", ylab = "",
main = "", key=list(cex.axis=.8, tick=FALSE, side = 4),
spacing.key = c(1,.5, -.5), axis.col = list(side = 3))
mtext("Descendant", side = 2, line = 3.5, cex = 1)
mtext("Ancestor", side = 3, line = 2.5, cex = 1)
pointEst <- causalXtreme::direct_lingam_search(Y)
famaData <- read.csv("~/Dropbox/confSetGraphs/code/rPkg/data_analysis/data/fama12_23.csv")
Y <- famaData[which(substr(famaData[,1], 1, 4) >= 2019), -1]
